{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8463522,"sourceType":"datasetVersion","datasetId":5045478},{"sourceId":8545597,"sourceType":"datasetVersion","datasetId":5045426}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basic Tutorial for GEORGE\n\nIn this notebook, we demonstrate a simple experiment comparing empirical risk minimization (ERM) and our method (GEORGE) on the MNIST dataset, using a small three-layer CNN model. In this simple example, GEORGE improves worst-case accuracy (i.e., the minimum accuracy over any subclass) compared to ERM. More sophisticated experiments are described in the blog post (and paper, coming soon). The notebook can be run with or without GPU support. For a script version rather than a notebook, see `stratification/run.py`.\n\nThere are four main sections to this notebook:\n1. **Setup**: Imports and setting up the dataset and model.\n2. **Train ERM Model**: we train an empirical risk minimization (ERM) model on the `superclass` labels.\n3. **Cluster Activations**: using the feature representation of the ERM model, we leverage dimensionality reduction and clustering techniques in order to estimate approximate `subclass` labels for each example.\n4. **Train \"GEORGE\" Model**: we train a new model that exploits the recovered `subclass` labels to improve worst-case performance on them, using group distributionally robust optimization (GDRO) \\[[Sagawa et al. (2020)](https://arxiv.org/abs/1911.08731)\\].","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup\n\n### 1.1 Imports and configuration setup\nBefore you start, make sure you have set up the repository correctly and installed all dependencies, as described in the README.\n\nAll training options are handled by a single `config` object. In this tutorial, we use the configuration provided in `demo_config.json`. To see how configuration files are defined, validated, and optionally modified via the command line, check out `stratification/utils/parse_args.py` and `stratification/utils/schema.py`.","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source and destination paths\nsource_folder = '/kaggle/input/hidden-stratification'\ndestination_folder = '/kaggle/working/hidden-stratification'\n\n# Create the destination folder if it doesn't exist\nos.makedirs(destination_folder, exist_ok=True)\n\n# Copy the contents of the source folder to the destination folder\nfor item in os.listdir(source_folder):\n    s = os.path.join(source_folder, item)\n    d = os.path.join(destination_folder, item)\n    if os.path.isdir(s):\n        shutil.copytree(s, d, dirs_exist_ok=True)  # Copy directories\n    else:\n        shutil.copy2(s, d)  # Copy files\n\nprint(f\"Contents of {source_folder} copied to {destination_folder}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:31:11.908392Z","iopub.execute_input":"2024-05-31T09:31:11.909094Z","iopub.status.idle":"2024-05-31T09:31:12.005095Z","shell.execute_reply.started":"2024-05-31T09:31:11.909063Z","shell.execute_reply":"2024-05-31T09:31:12.004079Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Contents of /kaggle/input/hidden-stratification copied to /kaggle/working/hidden-stratification\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source and destination paths\nsource_folder = '/kaggle/input/kaggle-json'\ndestination_folder = '/kaggle/working/kaggle-json'\n\n# Create the destination folder if it doesn't exist\nos.makedirs(destination_folder, exist_ok=True)\n\n# Copy the contents of the source folder to the destination folder\nfor item in os.listdir(source_folder):\n    s = os.path.join(source_folder, item)\n    d = os.path.join(destination_folder, item)\n    if os.path.isdir(s):\n        shutil.copytree(s, d, dirs_exist_ok=True)  # Copy directories\n    else:\n        shutil.copy2(s, d)  # Copy files\n\nprint(f\"Contents of {source_folder} copied to {destination_folder}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:31:14.343815Z","iopub.execute_input":"2024-05-31T09:31:14.344881Z","iopub.status.idle":"2024-05-31T09:31:14.353391Z","shell.execute_reply.started":"2024-05-31T09:31:14.344837Z","shell.execute_reply":"2024-05-31T09:31:14.352377Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Contents of /kaggle/input/kaggle-json copied to /kaggle/working/kaggle-json\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\n\n# Create the .kaggle directory if it doesn't exist\nos.makedirs('/root/.kaggle', exist_ok=True)\n\n# Move kaggle.json to the .kaggle directory\nshutil.move('/kaggle/working/kaggle-json/kaggle.json', '/root/.kaggle/kaggle.json')\n\n# Set the correct permissions for the file\nos.chmod('/root/.kaggle/kaggle.json', 0o600)\n\nprint(\"kaggle.json has been moved to /root/.kaggle and permissions have been set.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:31:16.841902Z","iopub.execute_input":"2024-05-31T09:31:16.842633Z","iopub.status.idle":"2024-05-31T09:31:16.848984Z","shell.execute_reply.started":"2024-05-31T09:31:16.842601Z","shell.execute_reply":"2024-05-31T09:31:16.847946Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"kaggle.json has been moved to /root/.kaggle and permissions have been set.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/hidden-stratification')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:31:19.872806Z","iopub.execute_input":"2024-05-31T09:31:19.873501Z","iopub.status.idle":"2024-05-31T09:31:19.877680Z","shell.execute_reply.started":"2024-05-31T09:31:19.873466Z","shell.execute_reply":"2024-05-31T09:31:19.876760Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:31:20.138640Z","iopub.execute_input":"2024-05-31T09:31:20.139006Z","iopub.status.idle":"2024-05-31T09:31:34.770242Z","shell.execute_reply.started":"2024-05-31T09:31:20.138976Z","shell.execute_reply":"2024-05-31T09:31:34.769024Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.29.3)\nRequirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (5.2.0)\nRequirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (6.28.0)\nRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (7.7.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.4.2)\nRequirement already satisfied: jsonargparse in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.29.0)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.20.0)\nRequirement already satisfied: jupyterlab in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (4.2.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (3.7.5)\nRequirement already satisfied: numba>=0.50 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.58.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.26.4)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (4.9.0.80)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.2.1)\nRequirement already satisfied: progress in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (1.6)\nRequirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (1.2.2)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (0.12.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.16.2)\nRequirement already satisfied: umap-learn>=0.4.5 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.5.6)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (0.40.2)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 2)) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 2)) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 2)) (4.66.4)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (0.2.1)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (1.8.0)\nRequirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (8.20.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (5.7.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (1.5.8)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (5.9.3)\nRequirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (6.3.3)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 3)) (5.9.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 4)) (0.2.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 4)) (3.6.6)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 4)) (3.0.9)\nRequirement already satisfied: PyYAML>=3.13 in /opt/conda/lib/python3.10/site-packages (from jsonargparse->-r requirements.txt (line 6)) (6.0.1)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->-r requirements.txt (line 7)) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->-r requirements.txt (line 7)) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->-r requirements.txt (line 7)) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->-r requirements.txt (line 7)) (0.16.2)\nRequirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 8)) (2.0.4)\nRequirement already satisfied: httpx>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 8)) (0.27.0)\nRequirement already satisfied: jinja2>=3.0.3 in /opt/conda/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 8)) (3.1.2)\nRequirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 8)) (2.2.5)\nRequirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 8)) (2.12.5)\nRequirement already satisfied: jupyterlab-server<3,>=2.27.1 in /opt/conda/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 8)) (2.27.2)\nRequirement already satisfied: notebook-shim>=0.2 in /opt/conda/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 8)) (0.2.3)\nRequirement already satisfied: tomli>=1.2.2 in /opt/conda/lib/python3.10/site-packages (from jupyterlab->-r requirements.txt (line 8)) (2.0.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 9)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 9)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 9)) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 9)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 9)) (2.9.0.post0)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.50->-r requirements.txt (line 10)) (0.41.1)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 13)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 13)) (2023.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->-r requirements.txt (line 15)) (1.11.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->-r requirements.txt (line 15)) (3.2.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 17)) (2.1.2)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.10/site-packages (from umap-learn>=0.4.5->-r requirements.txt (line 18)) (0.5.12)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->-r requirements.txt (line 19)) (6.11.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->-r requirements.txt (line 19)) (4.2.2)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from async-lru>=1.0.0->jupyterlab->-r requirements.txt (line 8)) (4.9.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 8)) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 8)) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 8)) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 8)) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 8)) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 8)) (0.14.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->-r requirements.txt (line 19)) (3.17.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (0.19.1)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (4.8.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.3->jupyterlab->-r requirements.txt (line 8)) (2.1.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 3)) (0.4)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (23.1.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.5.1)\nRequirement already satisfied: nbconvert>=6.4.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (6.4.5)\nRequirement already satisfied: nbformat>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (5.9.2)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (7.4.0)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.19.0)\nRequirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.18.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (1.7.0)\nRequirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 8)) (2.14.0)\nRequirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 8)) (0.9.14)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 9)) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (1.26.18)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 4)) (6.5.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 2)) (2.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (1.7.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r requirements.txt (line 17)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r requirements.txt (line 17)) (3.2.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r requirements.txt (line 17)) (2024.3.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (0.8.3)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (2.0.7)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.1.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.3.0)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (6.1.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.7.1)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.5.13)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (2.19.1)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 4)) (1.0.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (0.2.13)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (21.2.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 3)) (0.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision->-r requirements.txt (line 17)) (1.3.0)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (2.4)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (1.13)\nRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (1.16.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (0.5.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (2.21)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 8)) (2.8.19.20240106)\n\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -e .","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:31:46.133965Z","iopub.execute_input":"2024-05-31T09:31:46.134967Z","iopub.status.idle":"2024-05-31T09:32:11.219069Z","shell.execute_reply.started":"2024-05-31T09:31:46.134928Z","shell.execute_reply":"2024-05-31T09:32:11.218066Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/hidden-stratification\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: stratification\n  Attempting uninstall: stratification\n    Found existing installation: stratification 1.0\n    Uninstalling stratification-1.0:\n      Successfully uninstalled stratification-1.0\n  Running setup.py develop for stratification\nSuccessfully installed stratification-1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!python stratification/run.py configs/cifar.json exp_dir=checkpoints/new-experiment ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:32:11.220891Z","iopub.execute_input":"2024-05-31T09:32:11.221230Z","iopub.status.idle":"2024-05-31T09:59:29.581985Z","shell.execute_reply.started":"2024-05-31T09:32:11.221190Z","shell.execute_reply":"2024-05-31T09:59:29.580958Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"2024-05-31 09:32:23.414123: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 09:32:23.414185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 09:32:23.415644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nDataset URL: https://www.kaggle.com/datasets/fedesoriano/cifar100\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:30 | ETA 0:01 | Loss: 2.657 | R Loss: 2.984 | Acc: 18.550 | RW acc: 18.550 | R acc: 4.650 | RW R acc: 4.650 | TR acc: 2.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 2.363 | R Loss: 3.127 | Acc: 27.590 | RW acc: 27.590 | R acc: 2.000 | RW R acc: 2.000 | TR acc: 0.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 2.356 | R Loss: 2.825 | Acc: 26.980 | RW acc: 26.980 | R acc: 4.650 | RW R acc: 4.650 | TR acc: 2.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:09 | ETA 0:01 | Loss: 2.227 | R Loss: 3.032 | Acc: 30.640 | RW acc: 30.640 | R acc: 1.600 | RW R acc: 1.600 | TR acc: 0.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 2.224 | R Loss: 2.745 | Acc: 31.030 | RW acc: 31.030 | R acc: 7.800 | RW R acc: 7.800 | TR acc: 4.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 2.138 | R Loss: 2.744 | Acc: 33.520 | RW acc: 33.520 | R acc: 9.400 | RW R acc: 9.400 | TR acc: 1.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 2.142 | R Loss: 2.684 | Acc: 33.557 | RW acc: 33.557 | R acc: 9.500 | RW R acc: 9.500 | TR acc: 6.750  \nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 2.206 | R Loss: 2.903 | Acc: 32.420 | RW acc: 32.420 | R acc: 10.200 | RW R acc: 10.200 | TR acc: 0.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 2.061 | R Loss: 2.624 | Acc: 36.038 | RW acc: 36.038 | R acc: 11.950 | RW R acc: 11.950 | TR acc: 9.000 \nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 2.040 | R Loss: 2.849 | Acc: 37.360 | RW acc: 37.360 | R acc: 11.800 | RW R acc: 11.800 | TR acc: 1.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.992 | R Loss: 2.593 | Acc: 38.123 | RW acc: 38.123 | R acc: 13.800 | RW R acc: 13.800 | TR acc: 9.500 \nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.900 | R Loss: 2.872 | Acc: 40.610 | RW acc: 40.610 | R acc: 6.000 | RW R acc: 6.000 | TR acc: 1.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.932 | R Loss: 2.507 | Acc: 39.775 | RW acc: 39.775 | R acc: 15.100 | RW R acc: 15.100 | TR acc: 9.750 \nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.889 | R Loss: 2.838 | Acc: 41.180 | RW acc: 41.180 | R acc: 14.200 | RW R acc: 14.200 | TR acc: 5.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.876 | R Loss: 2.486 | Acc: 41.265 | RW acc: 41.265 | R acc: 15.600 | RW R acc: 15.600 | TR acc: 9.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.869 | R Loss: 2.784 | Acc: 41.990 | RW acc: 41.990 | R acc: 8.600 | RW R acc: 8.600 | TR acc: 3.000  \nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.818 | R Loss: 2.423 | Acc: 42.700 | RW acc: 42.700 | R acc: 18.450 | RW R acc: 18.450 | TR acc: 10.250\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.841 | R Loss: 2.624 | Acc: 43.400 | RW acc: 43.400 | R acc: 14.600 | RW R acc: 14.600 | TR acc: 4.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.777 | R Loss: 2.379 | Acc: 44.200 | RW acc: 44.200 | R acc: 21.600 | RW R acc: 21.600 | TR acc: 8.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.785 | R Loss: 2.419 | Acc: 44.910 | RW acc: 44.910 | R acc: 27.400 | RW R acc: 27.400 | TR acc: 7.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.740 | R Loss: 2.346 | Acc: 45.172 | RW acc: 45.172 | R acc: 21.450 | RW R acc: 21.450 | TR acc: 9.250 \nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.796 | R Loss: 2.666 | Acc: 44.770 | RW acc: 44.770 | R acc: 16.200 | RW R acc: 16.200 | TR acc: 9.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.697 | R Loss: 2.276 | Acc: 46.502 | RW acc: 46.502 | R acc: 24.000 | RW R acc: 24.000 | TR acc: 9.750 \nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.813 | R Loss: 2.708 | Acc: 44.780 | RW acc: 44.780 | R acc: 13.400 | RW R acc: 13.400 | TR acc: 5.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.665 | R Loss: 2.257 | Acc: 47.115 | RW acc: 47.115 | R acc: 23.050 | RW R acc: 23.050 | TR acc: 11.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.768 | R Loss: 2.809 | Acc: 45.490 | RW acc: 45.490 | R acc: 10.800 | RW R acc: 10.800 | TR acc: 3.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.632 | R Loss: 2.245 | Acc: 48.477 | RW acc: 48.477 | R acc: 24.250 | RW R acc: 24.250 | TR acc: 15.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.687 | R Loss: 2.726 | Acc: 47.930 | RW acc: 47.930 | R acc: 16.400 | RW R acc: 16.400 | TR acc: 5.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.592 | R Loss: 2.168 | Acc: 49.483 | RW acc: 49.483 | R acc: 25.950 | RW R acc: 25.950 | TR acc: 13.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.683 | R Loss: 2.522 | Acc: 47.490 | RW acc: 47.490 | R acc: 20.800 | RW R acc: 20.800 | TR acc: 3.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.563 | R Loss: 2.138 | Acc: 50.380 | RW acc: 50.380 | R acc: 27.250 | RW R acc: 27.250 | TR acc: 15.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.716 | R Loss: 3.018 | Acc: 46.960 | RW acc: 46.960 | R acc: 13.400 | RW R acc: 13.400 | TR acc: 7.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.538 | R Loss: 2.104 | Acc: 51.142 | RW acc: 51.142 | R acc: 29.750 | RW R acc: 29.750 | TR acc: 16.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.671 | R Loss: 2.592 | Acc: 49.090 | RW acc: 49.090 | R acc: 18.400 | RW R acc: 18.400 | TR acc: 8.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.507 | R Loss: 2.057 | Acc: 51.962 | RW acc: 51.962 | R acc: 30.750 | RW R acc: 30.750 | TR acc: 14.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.660 | R Loss: 2.726 | Acc: 49.010 | RW acc: 49.010 | R acc: 17.000 | RW R acc: 17.000 | TR acc: 10.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.484 | R Loss: 2.023 | Acc: 52.685 | RW acc: 52.685 | R acc: 32.000 | RW R acc: 32.000 | TR acc: 16.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.650 | R Loss: 2.430 | Acc: 50.250 | RW acc: 50.250 | R acc: 23.200 | RW R acc: 23.200 | TR acc: 11.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.465 | R Loss: 2.023 | Acc: 53.085 | RW acc: 53.085 | R acc: 31.450 | RW R acc: 31.450 | TR acc: 20.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.623 | R Loss: 2.425 | Acc: 50.040 | RW acc: 50.040 | R acc: 23.200 | RW R acc: 23.200 | TR acc: 11.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.425 | R Loss: 1.977 | Acc: 54.600 | RW acc: 54.600 | R acc: 34.500 | RW R acc: 34.500 | TR acc: 17.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.663 | R Loss: 2.786 | Acc: 49.400 | RW acc: 49.400 | R acc: 15.800 | RW R acc: 15.800 | TR acc: 7.000 \nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.403 | R Loss: 1.961 | Acc: 55.095 | RW acc: 55.095 | R acc: 34.700 | RW R acc: 34.700 | TR acc: 21.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.665 | R Loss: 2.677 | Acc: 49.740 | RW acc: 49.740 | R acc: 19.200 | RW R acc: 19.200 | TR acc: 8.000 \nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.376 | R Loss: 1.936 | Acc: 55.790 | RW acc: 55.790 | R acc: 35.000 | RW R acc: 35.000 | TR acc: 22.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.579 | R Loss: 2.459 | Acc: 51.890 | RW acc: 51.890 | R acc: 25.400 | RW R acc: 25.400 | TR acc: 15.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.357 | R Loss: 1.882 | Acc: 56.508 | RW acc: 56.508 | R acc: 37.450 | RW R acc: 37.450 | TR acc: 21.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.627 | R Loss: 2.804 | Acc: 51.060 | RW acc: 51.060 | R acc: 15.600 | RW R acc: 15.600 | TR acc: 8.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.348 | R Loss: 1.866 | Acc: 56.672 | RW acc: 56.672 | R acc: 38.200 | RW R acc: 38.200 | TR acc: 19.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.671 | R Loss: 2.687 | Acc: 50.720 | RW acc: 50.720 | R acc: 25.400 | RW R acc: 25.400 | TR acc: 13.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.316 | R Loss: 1.872 | Acc: 57.917 | RW acc: 57.917 | R acc: 38.100 | RW R acc: 38.100 | TR acc: 21.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.649 | R Loss: 2.515 | Acc: 50.760 | RW acc: 50.760 | R acc: 23.600 | RW R acc: 23.600 | TR acc: 13.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.298 | R Loss: 1.817 | Acc: 58.252 | RW acc: 58.252 | R acc: 38.500 | RW R acc: 38.500 | TR acc: 23.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.648 | R Loss: 3.087 | Acc: 50.900 | RW acc: 50.900 | R acc: 17.000 | RW R acc: 17.000 | TR acc: 7.000 \nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.276 | R Loss: 1.793 | Acc: 58.767 | RW acc: 58.767 | R acc: 40.350 | RW R acc: 40.350 | TR acc: 22.250\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.669 | R Loss: 2.843 | Acc: 50.800 | RW acc: 50.800 | R acc: 20.200 | RW R acc: 20.200 | TR acc: 15.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.251 | R Loss: 1.754 | Acc: 59.840 | RW acc: 59.840 | R acc: 40.700 | RW R acc: 40.700 | TR acc: 28.250\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.584 | R Loss: 2.684 | Acc: 52.550 | RW acc: 52.550 | R acc: 20.400 | RW R acc: 20.400 | TR acc: 9.000 \nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.236 | R Loss: 1.727 | Acc: 59.875 | RW acc: 59.875 | R acc: 41.500 | RW R acc: 41.500 | TR acc: 26.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.610 | R Loss: 2.852 | Acc: 51.990 | RW acc: 51.990 | R acc: 17.400 | RW R acc: 17.400 | TR acc: 13.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.211 | R Loss: 1.704 | Acc: 60.718 | RW acc: 60.718 | R acc: 42.600 | RW R acc: 42.600 | TR acc: 28.250\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.641 | R Loss: 3.167 | Acc: 52.360 | RW acc: 52.360 | R acc: 17.000 | RW R acc: 17.000 | TR acc: 10.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.206 | R Loss: 1.673 | Acc: 61.110 | RW acc: 61.110 | R acc: 44.150 | RW R acc: 44.150 | TR acc: 27.250\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.593 | R Loss: 2.943 | Acc: 52.700 | RW acc: 52.700 | R acc: 19.200 | RW R acc: 19.200 | TR acc: 13.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.176 | R Loss: 1.646 | Acc: 61.898 | RW acc: 61.898 | R acc: 45.200 | RW R acc: 45.200 | TR acc: 30.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.553 | R Loss: 2.505 | Acc: 54.080 | RW acc: 54.080 | R acc: 26.400 | RW R acc: 26.400 | TR acc: 11.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.162 | R Loss: 1.639 | Acc: 62.100 | RW acc: 62.100 | R acc: 44.950 | RW R acc: 44.950 | TR acc: 28.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.606 | R Loss: 2.655 | Acc: 53.180 | RW acc: 53.180 | R acc: 25.000 | RW R acc: 25.000 | TR acc: 11.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.150 | R Loss: 1.648 | Acc: 62.750 | RW acc: 62.750 | R acc: 46.150 | RW R acc: 46.150 | TR acc: 31.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.662 | R Loss: 2.926 | Acc: 51.730 | RW acc: 51.730 | R acc: 17.400 | RW R acc: 17.400 | TR acc: 10.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.124 | R Loss: 1.586 | Acc: 63.267 | RW acc: 63.267 | R acc: 46.300 | RW R acc: 46.300 | TR acc: 31.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.621 | R Loss: 3.057 | Acc: 53.620 | RW acc: 53.620 | R acc: 19.200 | RW R acc: 19.200 | TR acc: 8.000 \nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.113 | R Loss: 1.574 | Acc: 63.645 | RW acc: 63.645 | R acc: 47.250 | RW R acc: 47.250 | TR acc: 30.000\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.693 | R Loss: 3.221 | Acc: 52.160 | RW acc: 52.160 | R acc: 16.800 | RW R acc: 16.800 | TR acc: 13.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.092 | R Loss: 1.547 | Acc: 64.255 | RW acc: 64.255 | R acc: 47.950 | RW R acc: 47.950 | TR acc: 33.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.648 | R Loss: 3.052 | Acc: 52.390 | RW acc: 52.390 | R acc: 17.800 | RW R acc: 17.800 | TR acc: 14.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.069 | R Loss: 1.524 | Acc: 65.002 | RW acc: 65.002 | R acc: 49.000 | RW R acc: 49.000 | TR acc: 31.250\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.632 | R Loss: 2.791 | Acc: 52.970 | RW acc: 52.970 | R acc: 21.600 | RW R acc: 21.600 | TR acc: 11.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.056 | R Loss: 1.517 | Acc: 65.435 | RW acc: 65.435 | R acc: 49.500 | RW R acc: 49.500 | TR acc: 31.250\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.641 | R Loss: 2.910 | Acc: 53.470 | RW acc: 53.470 | R acc: 18.800 | RW R acc: 18.800 | TR acc: 10.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.043 | R Loss: 1.486 | Acc: 65.638 | RW acc: 65.638 | R acc: 49.550 | RW R acc: 49.550 | TR acc: 34.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.611 | R Loss: 3.021 | Acc: 54.430 | RW acc: 54.430 | R acc: 22.000 | RW R acc: 22.000 | TR acc: 14.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.020 | R Loss: 1.443 | Acc: 66.453 | RW acc: 66.453 | R acc: 52.250 | RW R acc: 52.250 | TR acc: 33.250\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.683 | R Loss: 2.942 | Acc: 53.260 | RW acc: 53.260 | R acc: 28.800 | RW R acc: 28.800 | TR acc: 19.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 1.011 | R Loss: 1.437 | Acc: 66.502 | RW acc: 66.502 | R acc: 51.100 | RW R acc: 51.100 | TR acc: 31.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.796 | R Loss: 3.174 | Acc: 51.950 | RW acc: 51.950 | R acc: 21.800 | RW R acc: 21.800 | TR acc: 13.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 0.996 | R Loss: 1.404 | Acc: 67.257 | RW acc: 67.257 | R acc: 51.800 | RW R acc: 51.800 | TR acc: 32.750\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.680 | R Loss: 3.008 | Acc: 53.450 | RW acc: 53.450 | R acc: 23.000 | RW R acc: 23.000 | TR acc: 13.000\nTraining |██████████████████████████████████████████████████| (313/313) Time 0:26 | ETA 0:01 | Loss: 0.972 | R Loss: 1.415 | Acc: 67.870 | RW acc: 67.870 | R acc: 53.450 | RW R acc: 53.450 | TR acc: 37.500\nEvaluation |██████████████████████████████████████████████████| (79/79) Time 0:08 | ETA 0:01 | Loss: 1.585 | R Loss: 2.529 | Acc: 55.370 | RW acc: 55.370 | R acc: 29.600 | RW R acc: 29.600 | TR acc: 14.000\nTraining |████████████████▎                                 | (102/313) Time 0:09 | ETA 0:18 | Loss: 0.938 | R Loss: 1.444 | Acc: 69.118 | RW acc: 69.118 | R acc: 53.168 | RW R acc: 53.168 | TR acc: 39.844^C\nTraceback (most recent call last):\n  File \"/kaggle/working/hidden-stratification/stratification/run.py\", line 88, in <module>\n    main()\n  File \"/kaggle/working/hidden-stratification/stratification/run.py\", line 49, in main\n    erm_dir = harness.classify(config['classification_config'], model, dataloaders,\n  File \"/kaggle/working/hidden-stratification/stratification/harness.py\", line 157, in classify\n    trainer.train(model, dataloaders['train'], dataloaders['val'], robust=robust)\n  File \"/kaggle/working/hidden-stratification/stratification/classification/george_classification.py\", line 179, in train\n    train_metrics, _ = self._run_epoch(model, train_dataloader, optimize=True,\n  File \"/kaggle/working/hidden-stratification/stratification/classification/george_classification.py\", line 347, in _run_epoch\n    metrics = self._compute_progress_metrics(losses, corrects, type_to_labels,\n  File \"/kaggle/working/hidden-stratification/stratification/classification/george_classification.py\", line 548, in _compute_progress_metrics\n    losses, counts = self.criterion.compute_group_avg(sample_losses, labels,\n  File \"/kaggle/working/hidden-stratification/stratification/classification/losses/loss_computer.py\", line 111, in compute_group_avg\n    group_range = torch.arange(num_groups).unsqueeze(1).long().to(group_idx.device)\nKeyboardInterrupt\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source and destination paths\ns = '/kaggle/working/hidden-stratification/checkpoints/new-experiment/run_2024-05-31_04-51-31_7407f3d6/experiment.log'\nd= '/kaggle/working/logs'\n\n# Create the destination folder if it doesn't exist\nos.makedirs(d, exist_ok=True)\nshutil.copy2(s, d)  # Copy files\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks('/kaggle/working/hidden-stratification/checkpoints')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport torch\n\nfrom stratification.harness import GEORGEHarness\nfrom stratification.utils.utils import set_seed, init_cuda\nfrom stratification.utils.parse_args import get_config\nfrom stratification.cluster.models.cluster import GaussianMixture\nfrom stratification.cluster.models.reduction import UMAPReducer\n\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\n\n# repository base directory\n#os.chdir(\"../\")\nREPO_DIR = os.getcwd()\nprint(REPO_DIR)\nwith open('configs/cifar.json', 'r') as f:\n    config = json.dumps(json.load(f))\nconfig = get_config([config])\n\nos.chdir(os.path.join(REPO_DIR, 'stratification'))\n\nuse_cuda_if_available = True  # change to True if you want to use CUDA\nuse_cuda = use_cuda_if_available and torch.cuda.is_available()\n# set seeds for reproducibility\nset_seed(config['seed'], use_cuda)\n# initialize CUDA, if available\n#config['allow_multigpu']\ninit_cuda(config['deterministic'],config['allow_multigpu'] );\nconfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Initialize GEORGEHarness\n\nThe `GEORGEHarness` is an object that handles the \"bookkeeping\" for each of the steps outlined in this tutorial, such as setting up experiment directories and loading/saving models.  Experiment files are stored in the base directory specified by `config['exp_dir']`. Each experiment run is stored in a subdirectory of this base directory whose filename is based on the (1) training method (ERM, GEORGE, etc.), (2) the timestamp, and (3) a random hash (to avoid collisions).","metadata":{}},{"cell_type":"code","source":"#\nharness = GEORGEHarness(config, use_cuda=use_cuda, log_format='simple')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Get Data and Model Architecture\n\nIn this tutorial, we'll use the MNIST dataset. In our case, the task will be to classify digits as < 5 or ≥ 5; these correspond to the two *superclasses*. The *subclasses* are the individual digits (0,1,2,3,4 are the subclasses of the first superclass, and 5,6,7,8,9 are the subclasses of the second superclass).\n\nWhen fetching the dataloaders and NN architecture, we can also specify the \"mode\" (training method), as one can specify different data and model options in the configuration for the different training methods.\n\nAdditional datasets and architectures can be added under `stratification/classification/datasets` and `stratification/classification/models`, respectively.","metadata":{}},{"cell_type":"code","source":"dataloaders = harness.get_dataloaders(config, mode='erm')\nnum_classes = dataloaders['train'].dataset.get_num_classes('superclass')\nmodel = harness.get_nn_model(config, num_classes=num_classes, mode='erm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Train ERM Model","metadata":{}},{"cell_type":"markdown","source":"We've already initialized our model, a simple CNN architecture. Let's print it out:","metadata":{}},{"cell_type":"code","source":"print('Model architecture:')\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we're ready to train a classifier! First we'll just train a standard classifier using empirical risk minimization (a fancy term for minimizing the average training loss). We'll print out both the overall accuracy and the true robust accuracy (i.e., the minimum accuracy on any subclass), along with the losses as well. The robust accuracy is the metric we are interested in maximizing. We train our models as though we don't know the subclass (digit) labels, in which case we can't actually measure the true robust accuracy. In reality, we do know the subclass labels for this dataset, so we'll measure per-subclass performance to see how well each method *really* does.","metadata":{}},{"cell_type":"code","source":"pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nsummary(model, (3, 28, 28))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"erm_dir = harness.classify(config['classification_config'],  model, dataloaders, 'erm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The overall test accuracy is around **92%**, but the robust accuracy is quite a bit lower at around **82%**. (Note: Results may vary based on random seed, platform, GPU use, etc.) Let's see if we can improve on this!","metadata":{}},{"cell_type":"markdown","source":"## 3. Cluster Model Activations\n\nNow, we'll cluster the data of each superclass, to try and automatically identify the subclasses. However, just clustering the raw data usually doesn't work that well - instead, we cluster in the *feature space* of a trained model. We just trained an ERM model on the task, so we'll now use this model to extract features which we use for clustering. Specifically, the features are the activations (outputs) of the penultimate layer (right before the classification layer).","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Initialize Cluster Model and Reduction Model\n\nThe clustering procedure consists of two steps:\n1. Dimensionality reduction of the activations (optional). If `reduction_model` is `None`, the raw activations are used.\n2. Fitting a separate cluster model on the reduced training activations of each superclass.\n\nWe'll use UMAP for dimensionality reduction, and we'll use Gaussian mixture model clustering. For simplicity, in this tutorial we fix the number of clusters per superclass to 5 (the true number of subclasses per superclass for this task). *Automatic* selection of the number of clusters based on unsupervised metrics (such as the Silhouette score) is also supported - in fact, our experiments in the blog post and paper are run using this automatic selection procedure, rather than pre-specifying the number of clusters.","metadata":{}},{"cell_type":"code","source":"# Dimensionality reduction model\nreduction_model = UMAPReducer(random_state=12345, n_components=2)\n# Clustering model\ncluster_model = GaussianMixture(covariance_type=\"full\", n_components=2, n_init=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Run `harness.reduce`\n\nNow, we use UMAP to dimensionality-reduce the activations to produce the \"features\" that we'll cluster.","metadata":{}},{"cell_type":"code","source":"reduction_dir = harness.reduce(config['reduction_config'], reduction_model,\n                               inputs_path=os.path.join(erm_dir, 'outputs.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3. Run `harness.cluster`\n\nNow, we cluster the aforementioned features.","metadata":{}},{"cell_type":"code","source":"# Now, we cluster the features separately for each superclass.\n# This step also generates and saves visualizations of the data, which we'll look at in the next part.\ncluster_dir = harness.cluster(config['cluster_config'], cluster_model,\n                              inputs_path=os.path.join(reduction_dir, 'outputs.pt'));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4. Visualizing the clusters","metadata":{}},{"cell_type":"markdown","source":"Now let's look at the clusters that we found. Since we apply UMAP to reduce to dimension 2, we can directly visualize the data in this two-dimensional \"feature space\" - see below! The first row corresponds to the first superclass (< 5, i.e. digits 0-4) and the second row corresponds to the second superclass (≥ 5, i.e. digits 5-9). On the left, we color each point by its assigned cluster label. On the right, we color each point by its actual subclass (i.e., which digit that datapoint is). As we can see, the individual subclasses are fairly easy to distinguish in feature space, and as a result *up to permutation* the clusters we find match up quite well with the actual subclasses.","metadata":{}},{"cell_type":"code","source":"viz_dir = os.path.join(cluster_dir, 'visualizations')\nfig, axarr = plt.subplots(2, 2, figsize=(18, 12), gridspec_kw={'wspace':0, 'hspace':0}, squeeze=True, dpi=300)\nfor i in range(2):\n    axarr[i, 0].imshow(mpimg.imread(os.path.join(viz_dir, f'train/group_{i}_cluster_viz.png')))\n    axarr[i, 0].axis('off')\n    axarr[i, 1].imshow(mpimg.imread(os.path.join(viz_dir, f'train/group_{i}_true_subclass_viz.png')))\n    axarr[i, 1].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Train Final (GEORGE) Model\n\nTraining the GEORGE model is simple. The only new thing to pass in is the `clusters.pt` path. This is a pickled dictionary saved by `harness.cluster` that contains the cluster labels assigned to the datapoints. These estimated cluster labels are used as a surrogate for the subclass labels. We now train a model to minimize the *worst-case* loss over the clusters using GDRO. Since the cluster assignments are similar to the true subclass labels up to permutation, we expect that this procedure should also improve worst-case accuracy on the true subclasses.\n\nAs before, we'll print out both the overall accuracy and true robust accuracy. We'll also print out the *estimated* robust accuracy, which is the minimum accuracy on any *cluster*. Unlike the true robust accuracy, this is something we can actually measure even when we don't know the true subclass labels. If the cluster labels are a good estimate of the true subclass labels, then this estimated robust accuracy should be a good estimate of the true robust accuracy.","metadata":{}},{"cell_type":"code","source":"set_seed(config['seed'], use_cuda)  # reset random state\n# Specify path to estimated subclass labels\ndataloaders = harness.get_dataloaders(\n    config, mode='george', subclass_labels=os.path.join(cluster_dir, 'clusters.pt'))\n# Initialize new model\nmodel = harness.get_nn_model(config, num_classes=num_classes, mode='george')\n\n# Train the final (GEORGE) model\ngeorge_dir = harness.classify(config['classification_config'], model, dataloaders,\n                              mode='george')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = [2,3]\nlist(a)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The overall accuracy is similar to that of the ERM model, but the robust accuracy has improved to around **89%**!\nIn addition, our estimate of the robust accuracy (**91%**) is quite close to the actual robust accuracy (and these two metrics remain close throughout the entire training run).\nAgain, results may vary somewhat - but on average across random seeds, the GEORGE model outperforms the ERM model in terms of robust accuracy.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nThis notebook demonstrates our framework (GEORGE) for estimating subclasses and improving worst-case subclass accuracy, on a simple \"toy\" example. Although the end-to-end performance gains are modest in this case, on more complex datasets the gains can be quite dramatic! See our blog post and paper for more details.","metadata":{}}]}