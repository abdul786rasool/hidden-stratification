{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4505237,"sourceType":"datasetVersion","datasetId":2633528},{"sourceId":8463522,"sourceType":"datasetVersion","datasetId":5045478},{"sourceId":9192610,"sourceType":"datasetVersion","datasetId":5045426}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basic Tutorial for GEORGE\n\nIn this notebook, we demonstrate a simple experiment comparing empirical risk minimization (ERM) and our method (GEORGE) on the MNIST dataset, using a small three-layer CNN model. In this simple example, GEORGE improves worst-case accuracy (i.e., the minimum accuracy over any subclass) compared to ERM. More sophisticated experiments are described in the blog post (and paper, coming soon). The notebook can be run with or without GPU support. For a script version rather than a notebook, see `stratification/run.py`.\n\nThere are four main sections to this notebook:\n1. **Setup**: Imports and setting up the dataset and model.\n2. **Train ERM Model**: we train an empirical risk minimization (ERM) model on the `superclass` labels.\n3. **Cluster Activations**: using the feature representation of the ERM model, we leverage dimensionality reduction and clustering techniques in order to estimate approximate `subclass` labels for each example.\n4. **Train \"GEORGE\" Model**: we train a new model that exploits the recovered `subclass` labels to improve worst-case performance on them, using group distributionally robust optimization (GDRO) \\[[Sagawa et al. (2020)](https://arxiv.org/abs/1911.08731)\\].","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup\n\n### 1.1 Imports and configuration setup\nBefore you start, make sure you have set up the repository correctly and installed all dependencies, as described in the README.\n\nAll training options are handled by a single `config` object. In this tutorial, we use the configuration provided in `demo_config.json`. To see how configuration files are defined, validated, and optionally modified via the command line, check out `stratification/utils/parse_args.py` and `stratification/utils/schema.py`.","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef print_gpu_memory():\n    # Initialize PyTorch CUDA context\n    torch.cuda.init()\n\n    # Get the current device\n    device = torch.cuda.current_device()\n\n    # Get memory stats\n    allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 3)\n    reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 3)\n    total_memory = torch.cuda.get_device_properties(device).total_memory / (1024 ** 3)\n    free_memory = total_memory - allocated_memory - reserved_memory\n\n    print(f\"Total GPU memory: {total_memory:.2f} GiB\")\n    print(f\"Allocated GPU memory: {allocated_memory:.2f} GiB\")\n    print(f\"Reserved but not allocated GPU memory: {reserved_memory:.2f} GiB\")\n    print(f\"Free GPU memory: {free_memory:.2f} GiB\")\n\nprint_gpu_memory()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:20:24.405965Z","iopub.execute_input":"2024-08-18T16:20:24.406860Z","iopub.status.idle":"2024-08-18T16:20:26.184886Z","shell.execute_reply.started":"2024-08-18T16:20:24.406824Z","shell.execute_reply":"2024-08-18T16:20:26.183932Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Total GPU memory: 14.74 GiB\nAllocated GPU memory: 0.00 GiB\nReserved but not allocated GPU memory: 0.00 GiB\nFree GPU memory: 14.74 GiB\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source and destination paths\nsource_folder = '/kaggle/input/hidden-stratification'\ndestination_folder = '/kaggle/working/hidden-stratification'\n\n# Create the destination folder if it doesn't exist\nos.makedirs(destination_folder, exist_ok=True)\n\n# Copy the contents of the source folder to the destination folder\nfor item in os.listdir(source_folder):\n    s = os.path.join(source_folder, item)\n    d = os.path.join(destination_folder, item)\n    if os.path.isdir(s):\n        shutil.copytree(s, d, dirs_exist_ok=True)  # Copy directories\n    else:\n        shutil.copy2(s, d)  # Copy files\n\nprint(f\"Contents of {source_folder} copied to {destination_folder}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:20:29.314374Z","iopub.execute_input":"2024-08-18T16:20:29.315332Z","iopub.status.idle":"2024-08-18T16:20:29.546802Z","shell.execute_reply.started":"2024-08-18T16:20:29.315290Z","shell.execute_reply":"2024-08-18T16:20:29.545858Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Contents of /kaggle/input/hidden-stratification copied to /kaggle/working/hidden-stratification\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source and destination paths\nsource_folder = '/kaggle/input/kaggle-json'\ndestination_folder = '/kaggle/working/kaggle-json'\n\n# Create the destination folder if it doesn't exist\nos.makedirs(destination_folder, exist_ok=True)\n\n# Copy the contents of the source folder to the destination folder\nfor item in os.listdir(source_folder):\n    s = os.path.join(source_folder, item)\n    d = os.path.join(destination_folder, item)\n    if os.path.isdir(s):\n        shutil.copytree(s, d, dirs_exist_ok=True)  # Copy directories\n    else:\n        shutil.copy2(s, d)  # Copy files\n\nprint(f\"Contents of {source_folder} copied to {destination_folder}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:20:30.789524Z","iopub.execute_input":"2024-08-18T16:20:30.790412Z","iopub.status.idle":"2024-08-18T16:20:30.799830Z","shell.execute_reply.started":"2024-08-18T16:20:30.790379Z","shell.execute_reply":"2024-08-18T16:20:30.798833Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Contents of /kaggle/input/kaggle-json copied to /kaggle/working/kaggle-json\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\n\n# Create the .kaggle directory if it doesn't exist\nos.makedirs('/root/.kaggle', exist_ok=True)\n\n# Move kaggle.json to the .kaggle directory\nshutil.move('/kaggle/working/kaggle-json/kaggle.json', '/root/.kaggle/kaggle.json')\n\n# Set the correct permissions for the file\nos.chmod('/root/.kaggle/kaggle.json', 0o600)\n\nprint(\"kaggle.json has been moved to /root/.kaggle and permissions have been set.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:20:32.242487Z","iopub.execute_input":"2024-08-18T16:20:32.243276Z","iopub.status.idle":"2024-08-18T16:20:32.249504Z","shell.execute_reply.started":"2024-08-18T16:20:32.243244Z","shell.execute_reply":"2024-08-18T16:20:32.248560Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"kaggle.json has been moved to /root/.kaggle and permissions have been set.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/hidden-stratification')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:20:37.491104Z","iopub.execute_input":"2024-08-18T16:20:37.491484Z","iopub.status.idle":"2024-08-18T16:20:37.495737Z","shell.execute_reply.started":"2024-08-18T16:20:37.491454Z","shell.execute_reply":"2024-08-18T16:20:37.494811Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -e .","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:20:51.638817Z","iopub.execute_input":"2024-08-18T16:20:51.639193Z","iopub.status.idle":"2024-08-18T16:21:08.642837Z","shell.execute_reply.started":"2024-08-18T16:20:51.639160Z","shell.execute_reply":"2024-08-18T16:21:08.641705Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/hidden-stratification\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hInstalling collected packages: stratification\n  Attempting uninstall: stratification\n    Found existing installation: stratification 1.0\n    Uninstalling stratification-1.0:\n      Successfully uninstalled stratification-1.0\n  Running setup.py develop for stratification\nSuccessfully installed stratification-1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!python stratification/run.py configs/waterbirds_george_config.json exp_dir=checkpoints/new-experiment mode='george' classification_config.optimizer_config.class_args.lr=0.005 classification_config.optimizer_config.class_args.weight_decay=1e-05 reduction_config.model='hardness' reduction_config.components=1 activations_dir='NONE' classification_config.num_epochs=1 classification_config.model='resnet50' classification_config.bit_pretrained=False cluster_dir='NONE' cluster_config.model='spectral' cluster_config.search_k=False cluster_config.k=2 allow_multigpu=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source and destination paths\ns = '/kaggle/working/hidden-stratification/checkpoints/new-experiment/run_2024-08-16_12-38-36_43136954/sp_subclass_gdro_2024-08-16_12-38-38_1d1c8db9/experiment.log'\nd= '/kaggle/working/logs'\n\n# Create the destination folder if it doesn't exist\nos.makedirs(d, exist_ok=True)\nshutil.copy2(s, d)  # Copy files\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source and destination paths\nsource_folder = '_947/visualizations/test'\ndestination_folder = '/kaggle/working/plots'\n\n# Create the destination folder if it doesn't exist\nos.makedirs(destination_folder, exist_ok=True)\n\n# Copy the contents of the source folder to the destination folder\nfor item in os.listdir(source_folder):\n    s = os.path.join(source_folder, item)\n    d = os.path.join(destination_folder, item)\n    if os.path.isdir(s):\n        shutil.copytree(s, d, dirs_exist_ok=True)  # Copy directories\n    else:\n        shutil.copy2(s, d)  # Copy files\n\nprint(f\"Contents of {source_folder} copied to {destination_folder}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks('/kaggle/working/hidden-stratification/checkpoints')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport torch\nimport os\nos.chdir('/kaggle/working/hidden-stratification')\nfrom stratification.harness import GEORGEHarness\nfrom stratification.utils.utils import set_seed, init_cuda\nfrom stratification.utils.parse_args import get_config\nfrom stratification.cluster.models.cluster import GaussianMixture\nfrom stratification.cluster.models.reduction import UMAPReducer\n\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\n\n# repository base directory\n#os.chdir(\"../\")\nREPO_DIR = os.getcwd()\nprint(REPO_DIR)\nwith open('configs/waterbirds_george_config.json', 'r') as f:\n    config = json.dumps(json.load(f))\nconfig = get_config([config])\n\nos.chdir(os.path.join(REPO_DIR, 'stratification'))\n\nuse_cuda_if_available = True  # change to True if you want to use CUDA\nuse_cuda = use_cuda_if_available and torch.cuda.is_available()\n# set seeds for reproducibility\nset_seed(config['seed'], use_cuda)\n# initialize CUDA, if available\n#config['allow_multigpu']\ninit_cuda(config['deterministic'],config['allow_multigpu'] );\nimport pprint\npprint.pprint(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config['classification_config']['bit_pretrained']=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(json.dumps(config,indent=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Initialize GEORGEHarness\n\nThe `GEORGEHarness` is an object that handles the \"bookkeeping\" for each of the steps outlined in this tutorial, such as setting up experiment directories and loading/saving models.  Experiment files are stored in the base directory specified by `config['exp_dir']`. Each experiment run is stored in a subdirectory of this base directory whose filename is based on the (1) training method (ERM, GEORGE, etc.), (2) the timestamp, and (3) a random hash (to avoid collisions).","metadata":{}},{"cell_type":"code","source":"#\nharness = GEORGEHarness(config, use_cuda=use_cuda, log_format='simple')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Get Data and Model Architecture\n\nIn this tutorial, we'll use the MNIST dataset. In our case, the task will be to classify digits as < 5 or ≥ 5; these correspond to the two *superclasses*. The *subclasses* are the individual digits (0,1,2,3,4 are the subclasses of the first superclass, and 5,6,7,8,9 are the subclasses of the second superclass).\n\nWhen fetching the dataloaders and NN architecture, we can also specify the \"mode\" (training method), as one can specify different data and model options in the configuration for the different training methods.\n\nAdditional datasets and architectures can be added under `stratification/classification/datasets` and `stratification/classification/models`, respectively.","metadata":{}},{"cell_type":"code","source":"dataloaders = harness.get_dataloaders(config, mode='erm')\nnum_classes = dataloaders['train'].dataset.get_num_classes('superclass')\nmodel = harness.get_nn_model(config, num_classes=num_classes, mode='erm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Train ERM Model","metadata":{}},{"cell_type":"markdown","source":"We've already initialized our model, a simple CNN architecture. Let's print it out:","metadata":{}},{"cell_type":"code","source":"print('Model architecture:')\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we're ready to train a classifier! First we'll just train a standard classifier using empirical risk minimization (a fancy term for minimizing the average training loss). We'll print out both the overall accuracy and the true robust accuracy (i.e., the minimum accuracy on any subclass), along with the losses as well. The robust accuracy is the metric we are interested in maximizing. We train our models as though we don't know the subclass (digit) labels, in which case we can't actually measure the true robust accuracy. In reality, we do know the subclass labels for this dataset, so we'll measure per-subclass performance to see how well each method *really* does.","metadata":{}},{"cell_type":"code","source":"pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nsummary(model, (3, 32, 32))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"erm_dir = harness.classify(config['classification_config'],  model, dataloaders, 'erm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The overall test accuracy is around **92%**, but the robust accuracy is quite a bit lower at around **82%**. (Note: Results may vary based on random seed, platform, GPU use, etc.) Let's see if we can improve on this!","metadata":{}},{"cell_type":"markdown","source":"## 3. Cluster Model Activations\n\nNow, we'll cluster the data of each superclass, to try and automatically identify the subclasses. However, just clustering the raw data usually doesn't work that well - instead, we cluster in the *feature space* of a trained model. We just trained an ERM model on the task, so we'll now use this model to extract features which we use for clustering. Specifically, the features are the activations (outputs) of the penultimate layer (right before the classification layer).","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Initialize Cluster Model and Reduction Model\n\nThe clustering procedure consists of two steps:\n1. Dimensionality reduction of the activations (optional). If `reduction_model` is `None`, the raw activations are used.\n2. Fitting a separate cluster model on the reduced training activations of each superclass.\n\nWe'll use UMAP for dimensionality reduction, and we'll use Gaussian mixture model clustering. For simplicity, in this tutorial we fix the number of clusters per superclass to 5 (the true number of subclasses per superclass for this task). *Automatic* selection of the number of clusters based on unsupervised metrics (such as the Silhouette score) is also supported - in fact, our experiments in the blog post and paper are run using this automatic selection procedure, rather than pre-specifying the number of clusters.","metadata":{}},{"cell_type":"code","source":"# Dimensionality reduction model\nreduction_model = UMAPReducer(random_state=12345, n_components=2)\n# Clustering model\ncluster_model = GaussianMixture(covariance_type=\"full\", n_components=2, n_init=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Run `harness.reduce`\n\nNow, we use UMAP to dimensionality-reduce the activations to produce the \"features\" that we'll cluster.","metadata":{}},{"cell_type":"code","source":"reduction_dir = harness.reduce(config['reduction_config'], reduction_model,\n                               inputs_path=os.path.join(erm_dir, 'outputs.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3. Run `harness.cluster`\n\nNow, we cluster the aforementioned features.","metadata":{}},{"cell_type":"code","source":"# Now, we cluster the features separately for each superclass.\n# This step also generates and saves visualizations of the data, which we'll look at in the next part.\ncluster_dir = harness.cluster(config['cluster_config'], cluster_model,\n                              inputs_path=os.path.join(reduction_dir, 'outputs.pt'));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4. Visualizing the clusters","metadata":{}},{"cell_type":"markdown","source":"Now let's look at the clusters that we found. Since we apply UMAP to reduce to dimension 2, we can directly visualize the data in this two-dimensional \"feature space\" - see below! The first row corresponds to the first superclass (< 5, i.e. digits 0-4) and the second row corresponds to the second superclass (≥ 5, i.e. digits 5-9). On the left, we color each point by its assigned cluster label. On the right, we color each point by its actual subclass (i.e., which digit that datapoint is). As we can see, the individual subclasses are fairly easy to distinguish in feature space, and as a result *up to permutation* the clusters we find match up quite well with the actual subclasses.","metadata":{}},{"cell_type":"code","source":"viz_dir = os.path.join(cluster_dir, 'visualizations')\nfig, axarr = plt.subplots(2, 2, figsize=(18, 12), gridspec_kw={'wspace':0, 'hspace':0}, squeeze=True, dpi=300)\nfor i in range(2):\n    axarr[i, 0].imshow(mpimg.imread(os.path.join(viz_dir, f'train/group_{i}_cluster_viz.png')))\n    axarr[i, 0].axis('off')\n    axarr[i, 1].imshow(mpimg.imread(os.path.join(viz_dir, f'train/group_{i}_true_subclass_viz.png')))\n    axarr[i, 1].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Train Final (GEORGE) Model\n\nTraining the GEORGE model is simple. The only new thing to pass in is the `clusters.pt` path. This is a pickled dictionary saved by `harness.cluster` that contains the cluster labels assigned to the datapoints. These estimated cluster labels are used as a surrogate for the subclass labels. We now train a model to minimize the *worst-case* loss over the clusters using GDRO. Since the cluster assignments are similar to the true subclass labels up to permutation, we expect that this procedure should also improve worst-case accuracy on the true subclasses.\n\nAs before, we'll print out both the overall accuracy and true robust accuracy. We'll also print out the *estimated* robust accuracy, which is the minimum accuracy on any *cluster*. Unlike the true robust accuracy, this is something we can actually measure even when we don't know the true subclass labels. If the cluster labels are a good estimate of the true subclass labels, then this estimated robust accuracy should be a good estimate of the true robust accuracy.","metadata":{}},{"cell_type":"code","source":"set_seed(config['seed'], use_cuda)  # reset random state\n# Specify path to estimated subclass labels\ndataloaders = harness.get_dataloaders(\n    config, mode='george', subclass_labels=os.path.join(cluster_dir, 'clusters.pt'))\n# Initialize new model\nmodel = harness.get_nn_model(config, num_classes=num_classes, mode='george')\n\n# Train the final (GEORGE) model\ngeorge_dir = harness.classify(config['classification_config'], model, dataloaders,\n                              mode='george')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = [2,3]\nlist(a)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The overall accuracy is similar to that of the ERM model, but the robust accuracy has improved to around **89%**!\nIn addition, our estimate of the robust accuracy (**91%**) is quite close to the actual robust accuracy (and these two metrics remain close throughout the entire training run).\nAgain, results may vary somewhat - but on average across random seeds, the GEORGE model outperforms the ERM model in terms of robust accuracy.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nThis notebook demonstrates our framework (GEORGE) for estimating subclasses and improving worst-case subclass accuracy, on a simple \"toy\" example. Although the end-to-end performance gains are modest in this case, on more complex datasets the gains can be quite dramatic! See our blog post and paper for more details.","metadata":{}}]}